{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### natural language processing\n",
    "#### nlp -대부분 text와 관련된 것\n",
    "#### nlp와 가장 많이 연관된 library: nltk\n",
    "#### nltk를 불러와야 하는데,\n",
    "#### 외부에서 파일을 불러올 때 os가 있으면 편하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### s에 담겨있는 것: string\n",
    "#### split을 쓰면 쪼개진다(띄어쓰기 단위로 쪼개진다)\n",
    "#### 쪼개놓은 것을 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'a man in the house' # untokenized string\n",
    "t = ['a', 'man', 'in', 'the', 'house'] # tokenized seqeuence of words as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nltk 안에 있는 text라는 함수를 쓰면, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: a man in the house...>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nltk.Text(t)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### s.split(): s를 tokenize한다.\n",
    "#### s.split()대신 t를 넣어도 된다.\n",
    "#### nltk에서 다루는 variable이 되었다고 생각하면 됨(nltk.text.Text)\n",
    "#### nltk.text.Text로 변환을 해야 더 복잡한 것들을 할 수 있다(string이나 list보다 처리하기 쉬운 변수가 되었다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=nltk.Text(s.split())\n",
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: a man in the house...>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.Text(s.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### os.getcwd(): 지금 켠 os에서 현재의 directory를 받아오는 함수\n",
    "#### +(단순히 string이 합쳐진다고 생각하면 됨) 현재 directory에 현재 포함되어 있는 06_01.txt 라는 파일을 열되,\n",
    "#### 이 파일은 utf8로 encoding 되어 있으니 이 encoding으로 지정해서 열어라(이걸로 지정해야 한글이 깨지지 않는다.)\n",
    "#### os.getcwd()+r'/06_01.txt'는 이 파일에 도달하는 컴퓨터 내의 full path라고 생각하면 된다.\n",
    "#### text에는 다양한 encoding이 있다.\n",
    "#### raw에 /06_01.txt 파일이 들어온다(encoding이 utf8로 되어있다는 전제 하에.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw = open(os.getcwd()+r'/06_01.txt', encoding = 'utf8').read()\n",
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3째줄: 75번째 글씨(character)까지 보자(주의: word 아님!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "1154507\n",
      "The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(raw))\n",
    "print(len(raw))\n",
    "print(raw[:75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nltk 속에 word_tokenize라는 함수가 있다.\n",
    "#### tokenize: 긴 string이 있을 때 단어 단위로 잘라주는 것(이걸 하기 위해 사용한 함수: split)\n",
    "#### nltk안에 tokenize를 할 수 있는 더 세련된 함수가 있다.\n",
    "#### word_tokenize의 input: string\n",
    "#### tokenize를 했을 때의 type = split을 했을 떄의 type = list\n",
    "#### print(len(tokens))의 값은 단어의 갯수와 비슷하다.\n",
    "#### 3째줄: 10번째 token까지 보자\n",
    "#### split으로 tokenize하면 콜론이나 쉼표가 앞 단어에 붙어 있는 형태로 끊어지는데,(무조건 띄어쓰기 단위로 끊어냄)\n",
    "#### nltk.word_tokenize로 tokenize하면 콜론과 쉼표도 다 따로 끊어낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "257726\n",
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(raw)\n",
    "print(type(tokens))\n",
    "print(len(tokens))\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nltk.Text라는 함수를 통해 nltk 전용 text인 변수(nltk.text.Text)가 되었다.\n",
    "#### nltk 전용 변수로 만들면 nltk 안에 있는 다양한 함수를 쓸 수 있다.\n",
    "#### 예: next.collocations() -> pair로 나온 output들: 같이 나올 확률이 높다는 뜻\n",
    "#### 대부분은 사람들의 성과 이름. 사람들의 선과 이름이 같이 나오는 것이 반복되니까 여기선 성과 이름이 collocation으로 인식됨.\n",
    "#### 숙어가 collocation으로 인식될 수도 있지.\n",
    "#### 다 pair로 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.text.Text'>\n",
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']\n",
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
      "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
      "great deal; young man; Nikodim Fomitch; Ilya Petrovitch; Project\n",
      "Gutenberg; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good\n",
      "heavens\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "print(type(text))\n",
    "print(text[:10])\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text.concordance(단어, 단어의 character 갯수  +  단어를 제외한 양쪽의 character의 갯수의 합, text 속 단어가 쓰인 25가지 경우)\n",
    "#### great가 보이는 것을 다 보여주는데, 문맥을 보고 싶지\n",
    "#### great를 중심으로 양쪽으로 great 포함해서 총 79개를 보면, 아~ 이런 상황에서 쓰이는 구나 알 수 있지\n",
    "#### 79를 줄이면 더 짧게 보인다.\n",
    "#### 25가지를 찾아라.(처음부터 25번째 sample까지 뽑아낼 수 있다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 162 matches:\n",
      "and were more frequent in periods of great strain . In 1859 he was allowed to r\n",
      "ndency and of late she had read with great interest a book she got through Mr. \n",
      " the bosom of her family ... . And a great deal more ... . Quite excusable , si\n",
      "that you had heard that Dounia had a great deal to put up with in the Svidrigra\n",
      "g will she has . Dounia can endure a great deal and even in the most difficult \n",
      " that letter she reproached him with great heat and indignation for the basenes\n",
      "putation ; they had seen and known a great deal more than Mr. Svidrigailov had \n",
      "n other people ’ s . In my opinion a great deal , a very great deal of all this\n",
      " In my opinion a great deal , a very great deal of all this was unnecessary ; b\n",
      " . He is a very busy man and is in a great hurry to get to Petersburg , so that\n",
      " me that , though he is not a man of great education , he is clever and seems t\n",
      " very well . Of course , there is no great love either on his side , or on hers\n",
      "tted the matter has been arranged in great haste . Besides he is a man of great\n",
      "great haste . Besides he is a man of great prudence and he will see , to be sur\n",
      "d that she is ready to put up with a great deal , if only their future relation\n",
      " off for Petersburg , where he has a great deal of business , and he wants to o\n",
      "a or I breathed a word to him of the great hopes we have of his helping us to p\n",
      "ites that ‘ Dounia can put up with a great deal. ’ I know that very well . I kn\n",
      "at , that ‘ Dounia can put up with a great deal. ’ If she could put up with Mr.\n",
      "it , she certainly can put up with a great deal . And now mother and she have t\n",
      "e young , and she was walking in the great heat bareheaded and with no parasol \n",
      "f the skirt , close to the waist : a great piece was rent and hanging loose . A\n",
      "ts or conversations . He worked with great intensity without sparing himself , \n",
      " uproarious and was reputed to be of great physical strength . One night , when\n",
      ". His legs felt suddenly heavy and a great drowsiness came upon him . He turned\n"
     ]
    }
   ],
   "source": [
    "text.concordance('great', 79, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "print(len('and were more frequent in periods of great strain . In 1859 he was allowed to r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### library 안에는 함수뿐만 아니라 data가 들어있기도 하다\n",
    "#### corpus가 담겨있는 폴더 내에서 gutenberg.raw 하면 melville-moby_dick.txt가 불러와진다.\n",
    "#### raw에 string이 들어와 있겠지\n",
    "#### FreqDist(raw STRING) -> tokenize를 하지 않고 raw string만 있더라도 그 속에서 어떤 character가 가장 빈번하게 나오는지를 볼 수 있다\n",
    "#### fdist에 most_common이라는 함수를 쓰면 가장 빈번히 사용된 것이 무엇인지를 보여준다.\n",
    "#### 가장 흔한 것은 space(198098회 나옴.))((총 상위 5개를 보여줌(5라는 숫자를 다른 숫자로 바꿀 수도 있겠지))\n",
    "\n",
    "#### ctrl + / 하면 한 번에 #을 달 수 있다(벗기는 것도 가능)\n",
    "\n",
    "### [ ch.lower() for ch in raw if ch.isalpha() ]의 의미: (list 안에 for 루프를 심는 것)\n",
    "#### (1) raw(text) 안의 character 갯수와 같은 횟수만큼 루프를 도는 데 그 루프 안에 if문이 있다.(ch: raw(text)의 글자(앞에서부터))\n",
    "#### (2) 인풋.isalpha() 함수: 그 인풋이 알파벳이냐고 질문(알파벳만 포함, 알파벳이 아닌 것(예: 숫자, 문장부호)이 포함되어 있으면 그냥 패스)\n",
    "#### (3) 알파벳이면 그것을 리스트로 만들 되, 그 속의 모든 알파벳을 소문자로 만들어서 리스트로 만들어라.(하지만 type: nltk.probability.FreqDist)\n",
    "\n",
    "#### 만든 list로 fregdist를 하면, 또 뭐가 나오겠지. 이젠 소문자만 있지, 더 우리가 원하는 깔끔한 모양으로.\n",
    "#### 어떤 철자가 이 소설에서 가장 많이 사용되는지 알 수 있게된다.(상위 5개만 한 거지)\n",
    "#### mostcommon이 주는 리스트는 튜플. 한 칸 당 2개의 정보가 들어있지.(type: list)\n",
    "#### for루프가 2개를 가지고 돈다.\n",
    "#### 마지막 줄: 만들어진 리스트가 5칸이니까 5번을 돌겠지. 이때 돌면서 character만 받아라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "\n",
      "\n",
      "<FreqDist with 82 samples and 1242990 outcomes>\n",
      "<class 'nltk.probability.FreqDist'>\n",
      "\n",
      "\n",
      "[(' ', 198098), ('e', 115855), ('t', 85539), ('a', 75266), ('o', 68338)]\n",
      "<class 'list'>\n",
      "5\n",
      "\n",
      "\n",
      "<FreqDist with 26 samples and 951971 outcomes>\n",
      "<class 'nltk.probability.FreqDist'>\n",
      "\n",
      "\n",
      "[('e', 117092), ('t', 87996), ('a', 77916), ('o', 69326), ('n', 65617)]\n",
      "<class 'list'>\n",
      "5\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['e', 't', 'a', 'o', 'n']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = nltk.corpus.gutenberg.raw('melville-moby_dick.txt')\n",
    "print(type(raw))\n",
    "print('\\n')\n",
    "\n",
    "fdist = nltk.FreqDist(raw)\n",
    "print(fdist)\n",
    "print(type(fdist))\n",
    "print('\\n')\n",
    "\n",
    "print(fdist.most_common(5))\n",
    "print(type(fdist.most_common(5)))\n",
    "print(len(fdist.most_common(5)))\n",
    "print('\\n')\n",
    "\n",
    "fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())\n",
    "print(fdist)\n",
    "print(type(fdist))\n",
    "print('\\n')\n",
    "\n",
    "print(fdist.most_common(5))\n",
    "print(type(fdist.most_common(5)))\n",
    "print(len(fdist.most_common(5)))\n",
    "print('\\n')\n",
    "\n",
    "[char for (char, count) in fdist.most_common(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zymosis',\n",
       " 'zymosterol',\n",
       " 'zymosthenic',\n",
       " 'zymotechnic',\n",
       " 'zymotechnical',\n",
       " 'zymotechnics',\n",
       " 'zymotechny',\n",
       " 'zymotic',\n",
       " 'zymotically',\n",
       " 'zymotize',\n",
       " 'zymotoxic',\n",
       " 'zymurgy',\n",
       " 'Zyrenian',\n",
       " 'Zyrian',\n",
       " 'Zyryan',\n",
       " 'zythem',\n",
       " 'Zythia',\n",
       " 'zythum',\n",
       " 'Zyzomys']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.words.words('en')[-20:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235886"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.words.words('en'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
